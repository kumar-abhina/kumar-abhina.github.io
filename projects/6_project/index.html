<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> One Shot Imitation Learning via Interaction Warping | Abhinav Kumar </title> <meta name="author" content="Abhinav Kumar"> <meta name="description" content="A detailed exploration of one-shot imitation learning using Interaction Warping for SE(3) manipulation tasks."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kumar-abhina.github.io/projects/6_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Abhinav</span> Kumar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">One Shot Imitation Learning via Interaction Warping</h1> <p class="post-description">A detailed exploration of one-shot imitation learning using Interaction Warping for SE(3) manipulation tasks.</p> </header> <article> <h1 id="introduction">Introduction</h1> <p><strong>Figure 1: The Mug Tree task.</strong></p> <p>In one-shot imitation learning, we are given a single demonstration of a desired manipulation behavior and we must find a policy that can reproduce the behavior in different situations. A classic example is the <em>Mug Tree</em> task, where a robot must grasp a mug and hang it on a tree by its handle. Given a single demonstration of grasping a mug and hanging it on a tree (top row of Figure 1), we want to obtain a policy that can successfully generalize across objects and poses (e.g. differently-shaped mugs and trees, as shown in the bottom row of Figure 1). This presents two key challenges:</p> <ul> <li>The demonstration must generalize to novel object instances (e.g. different mugs).</li> <li>The policy must reason in SE(3) rather than in SE(2), where the problem is much easier [1].</li> </ul> <p>To be successful in SE(3) manipulation, it is generally necessary to bias the model significantly toward the object manipulation domains in question. One popular approach is to establish a correspondence between points on the surfaces of the objects in the demonstration(s) and the same points on objects seen at test time. This is typically implemented using <strong>keypoints</strong>—point descriptors that encode the semantic location on an object’s surface and transfer well between novel instances [2, 3, 4]. For example, points on handles from different mugs should be assigned similar descriptors to aid in matching. A key challenge, therefore, becomes how to learn semantically meaningful keypoint descriptors. Early work used hand-coded feature labels [4], while more recent methods learn category-level object descriptor models during a pre-training step using implicit object models [5] or point models [2].</p> <p><em>7th Conference on Robot Learning (CoRL 2023), Atlanta, USA</em><br> <em>arXiv:2306.12392v2 [cs.RO] 4 Nov 2023</em></p> <p>This paper proposes a different approach to the point correspondence problem based on <strong>Coherent Point Drift (CPD)</strong> [6], a point-cloud warping algorithm. We call this method <strong>Interaction Warping</strong>. Using CPD, we train a shape-completion model to register a novel in-category object instance to a canonical object model in which the task has been defined via a single demonstration. The canonical task can then be projected into scenes with novel in-category objects by registering the new objects to the canonical models. Our method offers several advantages over previous approaches [2, 3, 4]:</p> <ol> <li>It performs better in terms of successfully executing a novel instance of a demonstrated task—both in simulation and on robotic hardware.</li> <li>It requires an order-of-magnitude fewer object instances to train each new object category (tens rather than hundreds).</li> <li>It is agnostic to the use of neural networks—the approach is based on CPD and PCA models (although neural networks can be incorporated).</li> </ol> <h1 id="related-work">Related Work</h1> <p>We draw on prior work in shape warping [7, 8] and imitation learning via keypoints [4].</p> <ul> <li> <strong>Shape Warping:</strong> Uses non-rigid point cloud registration [9] to align point clouds or meshes of objects, transferring robot skills across objects of different shapes. Our work is the first to use shape warping for relational object re-arrangement and for handling objects in arbitrary poses.</li> <li> <strong>Keypoints:</strong> Serve as a state abstraction that reduces objects to a set of task-specific keypoint poses. These keypoints help transfer robot actions. The novelty here is that our interaction points are automatically discovered and warped together with the object shape.</li> </ul> <p>Few-shot learning of manipulation policies has been explored using keypoint-based methods [4, 10, 11], which typically rely on human-annotated keypoints. More recent approaches have learned keypoints for tool affordances [12, 13, 14] and model-based RL [15]. Related work includes the learning of 2D [16] and 3D [5, 17, 18, 19] descriptor fields that provide semantic embeddings for arbitrary points, allowing keypoints to be matched across instances. We specifically compare our method to that of Simeonov et al. [5, 17] and show that our method requires fewer demonstrations. Other related approaches involve cross-attention between point clouds [2, 20] and pose estimation for precise object insertion [21].</p> <h1 id="background">Background</h1> <p><strong>Figure 2: Coherent Point Drift warping.</strong></p> <p><strong>Coherent Point Drift (CPD):</strong><br> Given two point clouds, (X^{(i)} \in \mathbb{R}^{n \times 3}) and (X^{(j)} \in \mathbb{R}^{m \times 3}), CPD finds a displacement (W_{i \to j} \in \mathbb{R}^{n \times 3}) that brings the points in (X^{(i)}) as close as possible (in an L2 sense) to the points in (X^{(j)}) [6]. CPD is a non-rigid registration method—each point in (X^{(i)}) can be translated independently. It minimizes the following cost function:</p> \[J(W_{i \to j}) = -\sum_{k=1}^{m} \log \sum_{l=1}^{n} \exp\left(-\frac{1}{2\sigma^2} \|X^{(i)}_l + (W_{i \to j})_l - X^{(j)}_k\|\right) + \frac{\alpha}{2} \, \phi(W_{i \to j}),\] <p>where (\phi(W_{i \to j})) is a regularization term that enforces coherent movement among nearby points.</p> <p><strong>Generative Object Modeling Using CPD:</strong><br> Assume we are given a set of point clouds ({X^{(1)}, \dots, X^{(K)}}) describing (K) object instances from a single category (e.g. various mug instances). Select a “canonical” object (X^{(C)}) ((C \in {1, 2, \dots, K})) and define displacement matrices (W_{C \to i} = \text{CPD}(X^{(C)}, X^{(i)})) for (i=1, \dots, K). Heuristically, we choose the most representative (X^{(C)}) (see Appendix A.2). We then form the flattened data matrix (\bar{W}<em>C = [\bar{W}</em>{C \to 1}, \dots, \bar{W}<em>{C \to K}]) and calculate a (d)-dimensional PCA projection matrix (W \in \mathbb{R}^{3n \times d}). This allows us to approximate novel in-category objects using a latent vector (v</em>{\text{novel}} \in \mathbb{R}^d) and compute a point cloud:</p> \[Y = X^{(C)} + \text{Reshape}(W v_{\text{novel}}),\] <p>where Reshape converts the vector back to an (n \times 3) matrix.</p> <p><strong>Shape Completion From Partial Point Clouds:</strong><br> To approximate a complete point cloud for an object seen partially [8], we solve:</p> \[L(Y) = D(Y, X(\text{partial})),\] <p>via gradient descent on (v). Here, (D(\cdot,\cdot)) is typically the one-sided Chamfer distance:</p> \[D\left(X^{(i)}, X^{(j)}\right) = \frac{1}{m} \sum_{k=1}^{m} \min_{l \in \{1, \dots, n\}} \|X^{(i)}_l - X^{(j)}_k\|^2.\] <p>Note that (X^{(i)} \in \mathbb{R}^{n \times 3}) and (X^{(j)} \in \mathbb{R}^{m \times 3}) may have different numbers of points.</p> <h1 id="interaction-warping">Interaction Warping</h1> <p>This section describes <strong>Interaction Warping (IW)</strong>, our proposed imitation method (see Figure 3). First, we train a set of category-level generative object models as described above. Then, given a single demonstration of a desired manipulation activity, we detect objects using off-the-shelf models. For each object matching a pre-trained model, we fit the model to obtain its pose and completed shape (Sections 4.1 and 4.2). Next, we identify interaction points on pairs of objects and correspond these points with those in the canonical object models. Finally, we reproduce the demonstration in a new scene by projecting the demonstrated interaction points onto completed object instances (Section 4.3).</p> <h2 id="41-joint-shape-and-pose-inference">4.1 Joint Shape and Pose Inference</h2> <p>To manipulate objects in SE(3), we jointly infer the pose and shape of an object represented by a point cloud (X(\text{partial})). We warp and transform the point cloud (Y \in \mathbb{R}^{n \times 3}) to minimize:</p> \[L(Y) = D(Y, X(\text{partial})) + \beta \max_k \|Y_k\|^2,\] <p>which is similar to Equation 3 but with an additional regularizer to keep the object’s size minimal (preventing oversized predicted meshes). We parameterize (Y) as a warped, scaled, rotated, and translated canonical point cloud:</p> \[Y = \left[(X^{(C)} + \text{Reshape}(W v)) \,\Big|\, \{z\}\right] \odot s \, R^T + t.\] <p>Here, (X^{(C)}) is a canonical point cloud, (v \in \mathbb{R}^d) parameterizes a warped shape, (s \in \mathbb{R}^3) represents scale, (R \in SO(3)) is a rotation matrix, and (t \in \mathbb{R}^3) represents translation. We optimize (L) with respect to (v), (s), and (t) using the Adam optimizer [36]. (R) is parameterized using an arbitrary matrix (\hat{R} \in \mathbb{R}^{2 \times 3}) followed by Gram-Schmidt orthogonalization (Algorithm 5) to yield a valid rotation matrix. This parameterization enables stable learning of rotations [37, 38]. We run the optimization with many random restarts (see Appendix A.4).</p> <h2 id="42-from-point-clouds-to-meshes">4.2 From Point Clouds to Meshes</h2> <p>While we infer object shape and pose by warping point clouds, collision checking and motion planning require meshes. We recover a warped mesh (M) by first warping the vertices of the canonical object. Since our model only warps points in (X^{(C)}) (Section 3), we include both the original mesh vertices and additional randomly sampled points on the canonical surface to ensure balanced warping. The first (V) points of (X^{(C)}) (which remain in order during warping) become the warped mesh vertices. These vertices, combined with the canonical faces, form the warped mesh (M).</p> <h2 id="43-transferring-robot-actions-via-interaction-points">4.3 Transferring Robot Actions via Interaction Points</h2> <p><strong>Figure 4:</strong></p> <ul> <li>(a) Contacts between a gripper and a bowl extracted from a demonstration.</li> <li>(b) Nearby points between a mug and a tree extracted from a demonstration of hanging the mug on the tree.</li> <li>(c) A virtual point (red) representing the branch of the tree intersecting the mug’s handle. The red point is anchored to the mug using (k) nearest neighbors (four shown in green).</li> </ul> <p>Consider a warped mug point cloud (Y) (from Equation 6). By tracking a point (Y_i) on the mug’s handle, we can align handles across mugs of different shapes and sizes. We call such points <strong>interaction points</strong>.</p> <h3 id="grasp-interaction-points">Grasp Interaction Points</h3> <p>We define grasp interaction points as pairs of contact points between the gripper and the object at grasp. Let (Y^{(A)}) and (M^{(A)}) denote the point cloud and mesh of the grasped object (obtained as in Sections 4.1 and 4.2), and let (M^{(G)}) be the gripper mesh with grasp pose (T_G). Using pybullet collision checking, we obtain (P) pairs of contact points (\left(p^{(A)}<em>j, p^{(G)}_j\right)) for (j=1,\dots,P). Since our model warps points in (Y^{(A)}), we identify indices (I_G = {i_1, \dots, i_P}) such that (Y^{(A)}</em>{i_j}) is the nearest neighbor of (p^{(A)}_j).</p> <h3 id="transferring-grasps">Transferring Grasps</h3> <p>For a new object with point cloud (Y^{(A’)}) (via Equation 6), we compute the new grasp by finding the transformation (T^<em>_G) that best aligns the pairs (\left(Y^{(A’)}_{i_j}, p^{(G)}_j\right)) for (j = 1, \dots, P). Shape warping preserves the order of points, ensuring a consistent correspondence between (Y^{(A)}) and (Y^{(A’)}). The predicted grasp (T^</em>_G) minimizes the pairwise distances using Horn et al.’s method [39] (see Figure 5a).</p> <h3 id="placement-interaction-points">Placement Interaction Points</h3> <p>For placement tasks (e.g. placing a mug on a mug-tree), interaction points are defined as pairs of nearby points between the two objects, even if they are not in direct contact. Let (Y^{(A)}) and (Y^{(B)}) be the inferred point clouds from a demonstration captured before the gripper opens. We select pairs of nearby points with an L2 distance less than (\delta):</p> \[\{(p^{(A)}, p^{(B)}) : \|p^{(A)} - p^{(B)}\| &lt; \delta\}.\] <p>Due to the large number of potential pairs, we sample a representative subset using farthest point sampling [40] and record the indices of (p^{(B)}_j) in (Y^{(B)}) as (I_P = {i_1, i_2, \dots, i_P}).</p> <p>Furthermore, we add (p^{(B)}<em>j) as virtual points into (Y^{(A)}) (illustrated in Figures 4b and 4c) to ensure proper alignment when there is no natural contact point. For each (p^{(B)}_j), we find (L) nearest neighbors ((n</em>{j,1}, \dots, n_{j,L})) in (Y^{(A)}) and anchor a virtual point (q^{(A)}_j) as follows:</p> \[q^{(A)}_j = \frac{1}{L} \sum_{k=1}^L \left( Y^{(A)}_{n_{j,k}} + \left( p^{(B)}_j - Y^{(A)}_{n_{j,k}} \right) \Delta_{j,k} \right) = p^{(B)}_j.\] <p>To transfer the placement, we save the neighbor indices (n_{j,k}) and displacements (\Delta_{j,k}).</p> <p>For new objects with point clouds (Y^{(A’)}) and (Y^{(B’)}), we compute the warped virtual points:</p> \[q^{(A')}_j = \frac{1}{L} \sum_{k=1}^L \left( Y^{(A')}_{n_{j,k}} + \Delta_{j,k} \right).\] <p>We then form point pairs (\left(q^{(A’)}<em>j, Y^{(B’)}</em>{i_j}\right)) for (j=1,\dots,P) and determine the optimal transformation (T^*_P) for placing object (A’) onto object (B’) (see Figure 5b).</p> <h1 id="experiments">Experiments</h1> <p>We evaluate both the perception and imitation learning capabilities of Interaction Warping.</p> <p>In <strong>Section 5.1</strong>, we perform three object re-arrangement tasks with previously unseen objects, both in simulation and on a physical robot. In <strong>Section 5.2</strong>, we demonstrate that our system can propose grasps in a cluttered kitchen setting from a single RGB-D image.</p> <p>We use ShapeNet [41] for per-category (mug, bowl, bottle, and box) object pre-training (required by our method and all baselines). Synthetic mug-tree meshes from [17] are also used. Our method (IW) requires only 10 training examples per class, whereas all baselines use 200 examples. All training meshes are aligned in a canonical pose.</p> <h2 id="51-object-re-arrangement">5.1 Object Re-arrangement</h2> <p><strong>Setup:</strong><br> We use an open-source simulated environment with three tasks:</p> <ul> <li>Mug on a mug-tree</li> <li>Bowl on a mug</li> <li>Bottle in a container [17]</li> </ul> <p>Given a segmented point cloud of the initial scene, the goal is to predict the pose of the child object relative to the parent object (e.g., the mug relative to the mug-tree). A successful action places the object on a rack or in a container so that it does not fall, while avoiding collisions. The simulation does not test grasp prediction. These tasks are demonstrated with objects unseen during pre-training. In Section 4.3, we described how IW uses a single demonstration; when multiple demonstrations are available, IW selects the most informative one using training prediction error (see Appendix A.5).</p> <h1 id="conclusion">Conclusion</h1> <p>In this work, we introduced <strong>Interaction Warping</strong>, a novel method for one-shot imitation learning that leverages point cloud registration techniques such as CPD and PCA-based generative models. Our approach significantly reduces the number of required demonstrations and generalizes well across novel object instances in challenging SE(3) manipulation tasks.</p> <hr> <p><em>This paper was presented at the 7th Conference on Robot Learning (CoRL 2023) in Atlanta, USA.</em></p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Abhinav Kumar. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"This is My updated Resume",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-galaxy-of-achievers-award",title:"Galaxy Of achievers Award",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"projects-structure-from-motion",title:"Structure From Motion",description:"3D reconstruction based on sequence of images",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-autonomous-navigation-and-slam-with-turtlebot3",title:"Autonomous navigation and SLAM with turtlebot3",description:"a project with autonmous navigation of slam and apriltag detction for hence making a resuce robot for recon missions in a simulated diaster zone",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-homography-estimation-and-image-stitching",title:"Homography Estimation and Image Stitching",description:"Panoramic image stitching set of consecutive images using feature matchimng and Homography estimation",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-navigation-stack-using-imu-and-rtk-gps-with-ros",title:"Navigation stack using IMU and RTK-GPS with ROS.",description:"ROS drivers for navigation stack based on IMU and GPS sensors",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-panda-arm-control-with-rl-algorithms",title:"Panda Arm Control with RL Algorithms",description:"Deep Reinforcement Learning Tasks on Panda Arm.",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-one-shot-imitation-learning-via-interaction-warping",title:"One Shot Imitation Learning via Interaction Warping",description:"A detailed exploration of one-shot imitation learning using Interaction Warping for SE(3) manipulation tasks.",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-pythonic-manipulation-stack-for-the-unitree-g1-robot",title:"Pythonic Manipulation Stack for the Unitree G1 Robot",description:"Built a custom Python control stack for the Unitree G1 Robot for Inverse kinematics solver, Controller and Motion Planner",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6B%75%6D%61%72.%61%62%68%69%6E%61@%6E%6F%72%74%68%65%61%73%74%65%72%6E.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/kumar-abhina","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/Abhinav Kumar","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>